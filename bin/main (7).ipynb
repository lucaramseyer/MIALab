{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f6bc4-da9b-443e-bcdf-8858dfa98619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import cnn\n",
    "\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "    \n",
    "# 512x512 color images in 11 classes\n",
    "train_dataset = cnn.CustomImageDataset(\"annotations_file_training.csv\", \"All_training\", transform=torchvision.transforms.Resize((512,512)))\n",
    "\n",
    "validation_dataset = cnn.CustomImageDataset(\"annotations_file_validation.csv\", \"All_validation\", transform=torchvision.transforms.Resize((512,512)))\n",
    "\n",
    "test_dataset = cnn.CustomImageDataset(\"annotations_file_evaluation.csv\", \"All_evaluation\", transform=torchvision.transforms.Resize((512,512)))\n",
    "                                            \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "                                          \n",
    "classes = ('bread', 'dairy' 'products', 'dessert', 'eggs', 'fried food', 'meat', 'noodles-pasta', 'rice', 'seafood', 'soup', 'vegetables-fruits')\n",
    "\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "cnn.imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# Let's train that bitch\n",
    "print(\"Getting ready to train\")\n",
    "model = cnn.ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "steps_train = len(train_loader)\n",
    "steps_val = len(validation_loader)\n",
    "train_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    mean_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        mean_loss = mean_loss + loss.item()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{steps_train}], Loss: {loss.item():.4f}', end='\\r')\n",
    "        if (i + 1) == len(train_loader):\n",
    "            mean_loss = mean_loss/steps_train\n",
    "            train_loss.append(mean_loss)\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{steps_train}], Train Loss: {train_loss[epoch]:.4f}', end='\\r')\n",
    "            \n",
    "    mean_loss = 0\n",
    "    for i, (images, labels) in enumerate(validation_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        mean_loss = mean_loss + loss.item()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{steps_train}/{steps_train}], Train Loss: {train_loss[epoch]:.4f}, Step [{i + 1}/{steps_val}], Loss: {loss.item():.4f}', end='\\r')\n",
    "        if (i + 1) == len(validation_loader):\n",
    "            mean_loss = mean_loss/steps_val\n",
    "            validation_loss.append(mean_loss)\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{steps_train}/{steps_train}], Train Loss: {train_loss[epoch]:.4f}, Step [{i + 1}/{steps_val}], Validation Loss: {validation_loss[epoch]:.4f}')\n",
    "\n",
    "            \n",
    "# loss plots\n",
    "os.mkdir(\"./cnn_ep{}_bs{}_lr{}\".format(num_epochs, batch_size, learning_rate))\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"ep{} bs{} lr{}\".format(num_epochs, batch_size, learning_rate))\n",
    "plt.plot(train_loss, color='orange', linestyle='-', label='train loss')\n",
    "plt.plot(validation_loss, color='red', linestyle='-', label='validataion loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"./cnn_ep{}_bs{}_lr{}/loss_ep{}_bs{}_lr{}.png\".format(num_epochs, batch_size, learning_rate, num_epochs, batch_size, learning_rate))\n",
    "plt.show()            \n",
    "\n",
    "print('Finished Training')\n",
    "PATH = \"./cnn_ep{}_bs{}_lr{}/cnn_ep{}_bs{}_lr{}.pth\".format(num_epochs, batch_size, learning_rate, num_epochs, batch_size, learning_rate)\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "header = ['train loss', 'validation loss']\n",
    "with open(\"./cnn_ep{}_bs{}_lr{}/cnn_ep{}_bs{}_lr{}.csv\".format(num_epochs, batch_size, learning_rate, num_epochs, batch_size, learning_rate), 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    for i in range(num_epochs):\n",
    "        writer.writerow([train_loss[i], validation_loss[i]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e52240-9e6a-4046-b043-482a5269ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "# Only run this if you want to thest a model other than the one just trained\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(sys.argv[0]), '..'))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cnn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classes = ('bread', 'dairy' 'products', 'dessert', 'eggs', 'fried food', 'meat', 'noodles-pasta', 'rice', 'seafood', 'soup', 'vegetables-fruits')\n",
    "\n",
    "test_dataset = cnn.CustomImageDataset(\"annotations_file_evaluation.csv\", \"All_evaluation\", transform=torchvision.transforms.Resize((512,512)))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)  \n",
    "\n",
    "\n",
    "\n",
    "model = cnn.ConvNet().to(device)\n",
    "model.load_state_dict(torch.load('cnn_ep20_bs8_lr0,001.pth'))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6302bc5a-4832-4267-afad-f9a2503610bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(11)]\n",
    "    n_class_samples = [0 for i in range(11)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images.float())\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(11):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c40cc73-4af3-4a82-89f5-d95a93fe9072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the heatmaps\n",
    "import cnn\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import importlib\n",
    "import numpy as np\n",
    "importlib.reload(cnn)\n",
    "\n",
    "\n",
    "for i, (images, labels) in enumerate(test_loader):\n",
    "    images = images.to(device)\n",
    "    output = model(images.float(), feature_conv=True)\n",
    "    \n",
    "    cam = cnn.returnCAM(output, model.fc1.weight, model.fc2.weight, model.fc3.weight, list(range(0, 11)))\n",
    "    print(len(cam))\n",
    " \n",
    "    #files = os.listdir('./All_evaluation')\n",
    "  \n",
    "    images = torchvision.utils.make_grid(images.cpu())\n",
    "    images = np.transpose(images, (1, 2, 0))\n",
    "    fig, axs = plt.subplots(2, 6, figsize=(20,6))\n",
    "    \n",
    "    for i in range(12):\n",
    "        axs[int(i>=6), i%6].imshow(images)\n",
    "        axs[int(i>=6), i%6].axis('off')\n",
    "        if i < 11:\n",
    "            heatmap = cam[i]\n",
    "            heatmap = heatmap.cpu()\n",
    "            heatmap = heatmap.detach().numpy()\n",
    "            heatmap = np.squeeze(heatmap)\n",
    "            axs[int(i>=6), i%6].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "            axs[int(i>=6), i%6].set_title(classes[i])\n",
    "            \n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3634522f-c65c-4e03-a36b-4ccb33551a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = cnn.CustomImageDataset(\"./Whatsapp/annotations.csv\", \"Whatsapp\", transform=torchvision.transforms.Resize((512,512)))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False) \n",
    "for i, (images, labels) in enumerate(test_loader):\n",
    "    print(i)\n",
    "    images = images.to(device)\n",
    "    output = model(images.float())\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    print(classes[predicted[0]])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
